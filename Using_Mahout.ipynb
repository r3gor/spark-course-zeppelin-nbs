{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The [Apache Mahout](http://mahout.apache.org/)â„¢ project's goal is to build an environment for quickly creating scalable performant machine learning applications.\n",
    "\n",
    "#### Apache Mahout software provides three major features:\n",
    "\n",
    "- A simple and extensible programming environment and framework for building scalable algorithms\n",
    "- A wide variety of premade algorithms for Scala + Apache Spark, H2O, Apache Flink\n",
    "- Samsara, a vector math experimentation environment with R-like syntax which works at scale\n",
    "\n",
    "#### In other words:\n",
    "\n",
    "*Apache Mahout provides a unified API for quickly creating machine learning algorithms on a variety of engines.*\n",
    "\n",
    "#### Getting Started\n",
    "\n",
    "Apache Mahout is a collection of Libraries that enhance Apache Flink, Apache Spark, and others. Currently Zeppelin support the Flink and Spark Engines. A convenience script is provided to setup the nessecary imports and configurations to run Mahout on Spark and Flink. \n",
    "\n",
    "We can use Apache Mahout's R-Like Domain Specific Language (DSL) inline with native Flink or Spark code.  We must however, first declare a few imports that are different for Spark and Flink\n",
    "\n",
    "__References:__\n",
    "\n",
    "[Mahout-Samsara's In-Core Linear Algebra DSL Reference](http://mahout.apache.org/users/environment/in-core-reference.html)\n",
    "[Mahout-Samsara's Distributed Linear Algebra DSL Reference](http://mahout.apache.org/users/environment/out-of-core-reference.html)\n",
    "[Getting Started with the Mahout-Samsara Shell](http://mahout.apache.org/users/sparkbindings/play-with-shell.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Installing\" the Apache Mahout dependencies and configuring a new Spark and Flink interpreter\n",
    "\n",
    "The following two paragraphs are convenience paragraphs. You **only need to run them once** to create two new interpreters `%spark.mahout` and `%flink.mahout`. These are intended for users who don't have Apache Mahout already installed. They assume you started Apache Zeppelin from the top level directory or from the bin.  You can tell which one is you by weather you started Zeppelin by typing `./zeppelin-daemon.sh start` or `bin/zeppelin-daemon.sh start`.  If you started Zeppelin from somewhere else you will also need to run them from the command line.\n",
    "\n",
    "They both run a python script which may be found at `ZEPPELIN_HOME/scripts/mahout/add_mahout.py`\n",
    "\n",
    "In short this script:\n",
    "- Downloads Apache Mahout\n",
    "- Creates a new Flink interpreter with dependencies.\n",
    "- Creates a new Spark interpreter with dependencies and modified configuration to use Kryo serialization.\n",
    "\n",
    "__You only need to run this script once ever.__ (Maybe again if for some reason you delete `conf/interpreter.json`) \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%sh\n",
    "\n",
    "python ../scripts/mahout/add_mahout.py"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%sh\n",
    "python scripts/mahout/add_mahout_interpreters.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the interpreters are created you will need to 'bind' them by clicking on the little gear in the top right corner, scrolling to the top, and clicking on `mahoutFlink` and `mahoutSpark` so that they are highlighted in blue.\n",
    "\n",
    "#### Running Mahout code\n",
    "\n",
    "You will need to import certain libraries, and declare the _Mahout Distributed Context_ when you first start your notebook using the interpreters. \n",
    "\n",
    "If using Apache Flink the code you need to run is:\n",
    "```scala\n",
    "%flinkMahout\n",
    "\n",
    "import org.apache.flink.api.scala._\n",
    "import org.apache.mahout.math.drm._\n",
    "import org.apache.mahout.math.drm.RLikeDrmOps._\n",
    "import org.apache.mahout.flinkbindings._\n",
    "import org.apache.mahout.math._\n",
    "import scalabindings._\n",
    "import RLikeOps._\n",
    "\n",
    "\n",
    "implicit val ctx = new FlinkDistributedContext(benv)\n",
    "```\n",
    "\n",
    "If using Apache Spark the code you need to run is\n",
    "```scala\n",
    "%sparkMahout\n",
    "\n",
    "import org.apache.mahout.math._\n",
    "import org.apache.mahout.math.scalabindings._\n",
    "import org.apache.mahout.math.drm._\n",
    "import org.apache.mahout.math.scalabindings.RLikeOps._\n",
    "import org.apache.mahout.math.drm.RLikeDrmOps._\n",
    "import org.apache.mahout.sparkbindings._\n",
    "\n",
    "implicit val sdc: org.apache.mahout.sparkbindings.SparkDistributedContext = sc2sdc(sc)\n",
    "```\n",
    "\n",
    "__Note: For Apache Mahout on Apache Spark you must be running Spark 1.5.x or 1.6.x.  We are working hard on supporting Spark 2.0__\n",
    "In the meantime, feel free to play with Mahout on Flink and then simple _copy and paste your Mahout code to Spark once it is supported!_\n",
    "\n",
    "### A Side by Side Example\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%flinkMahout\n",
    "\n",
    "// Imports and creating the distributed context, similar but not exactly the same ///////////////////////////////////////////\n",
    "import org.apache.flink.api.scala._\n",
    "import org.apache.mahout.math.drm._\n",
    "import org.apache.mahout.math.drm.RLikeDrmOps._\n",
    "import org.apache.mahout.flinkbindings._\n",
    "import org.apache.mahout.math._\n",
    "import scalabindings._\n",
    "import RLikeOps._\n",
    "\n",
    "\n",
    "implicit val ctx = new FlinkDistributedContext(benv)\n",
    "\n",
    "// CODE IS EXACTLY THE SAME FROM HERE ON - R-Like DSL ////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "val drmData = drmParallelize(dense(\n",
    "  (2, 2, 10.5, 10, 29.509541),  // Apple Cinnamon Cheerios\n",
    "  (1, 2, 12,   12, 18.042851),  // Cap'n'Crunch\n",
    "  (1, 1, 12,   13, 22.736446),  // Cocoa Puffs\n",
    "  (2, 1, 11,   13, 32.207582),  // Froot Loops\n",
    "  (1, 2, 12,   11, 21.871292),  // Honey Graham Ohs\n",
    "  (2, 1, 16,   8,  36.187559),  // Wheaties Honey Gold\n",
    "  (6, 2, 17,   1,  50.764999),  // Cheerios\n",
    "  (3, 2, 13,   7,  40.400208),  // Clusters\n",
    "  (3, 3, 13,   4,  45.811716)), numPartitions = 2)\n",
    "  \n",
    "drmData.collect(::, 0 until 4)\n",
    "\n",
    "val drmX = drmData(::, 0 until 4)\n",
    "val y = drmData.collect(::, 4)\n",
    "val drmXtX = drmX.t %*% drmX\n",
    "val drmXty = drmX.t %*% y\n",
    "\n",
    "\n",
    "val XtX = drmXtX.collect\n",
    "val Xty = drmXty.collect(::, 0)\n",
    "val beta = solve(XtX, Xty)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%sparkMahout\n",
    "\n",
    "// Imports and creating the distributed context, similar but not exactly the same ///////////////////////////////////////////\n",
    "\n",
    "import org.apache.mahout.math._\n",
    "import org.apache.mahout.math.scalabindings._\n",
    "import org.apache.mahout.math.drm._\n",
    "import org.apache.mahout.math.scalabindings.RLikeOps._\n",
    "import org.apache.mahout.math.drm.RLikeDrmOps._\n",
    "import org.apache.mahout.sparkbindings._\n",
    "\n",
    "implicit val sdc: org.apache.mahout.sparkbindings.SparkDistributedContext = sc2sdc(sc)\n",
    "\n",
    "\n",
    "// CODE IS EXACTLY THE SAME FROM HERE ON - R-Like DSL ////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "val drmData = drmParallelize(dense(\n",
    "  (2, 2, 10.5, 10, 29.509541),  // Apple Cinnamon Cheerios\n",
    "  (1, 2, 12,   12, 18.042851),  // Cap'n'Crunch\n",
    "  (1, 1, 12,   13, 22.736446),  // Cocoa Puffs\n",
    "  (2, 1, 11,   13, 32.207582),  // Froot Loops\n",
    "  (1, 2, 12,   11, 21.871292),  // Honey Graham Ohs\n",
    "  (2, 1, 16,   8,  36.187559),  // Wheaties Honey Gold\n",
    "  (6, 2, 17,   1,  50.764999),  // Cheerios\n",
    "  (3, 2, 13,   7,  40.400208),  // Clusters\n",
    "  (3, 3, 13,   4,  45.811716)), numPartitions = 2)\n",
    "  \n",
    "drmData.collect(::, 0 until 4)\n",
    "\n",
    "val drmX = drmData(::, 0 until 4)\n",
    "val y = drmData.collect(::, 4)\n",
    "val drmXtX = drmX.t %*% drmX\n",
    "val drmXty = drmX.t %*% y\n",
    "\n",
    "\n",
    "val XtX = drmXtX.collect\n",
    "val Xty = drmXty.collect(::, 0)\n",
    "val beta = solve(XtX, Xty)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking advantage of Zeppelin Resource Pools\n",
    "\n",
    "One of the major motivations for integrating Apache Mahout with Apache Zeppelin was the many benefits that come from leveraging the resource pools.  A resource pool is a block of memory that can be acccessed by all interpreters and is useful for sharing small variables between the interpreters. \n",
    "\n",
    "The Spark interpreter has a simple interface for accessing the ResourcePools, the Flink interface is less documented but can be reverse engineered from code (thanks open source!)\n",
    "\n",
    "\n",
    "Collect betas from Spark and Flink- compare in Python\n",
    "\n",
    "Create Matrix in Flink and Spark - visualize with R"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%flinkMahout\n",
    "\n",
    "import org.apache.zeppelin.interpreter.InterpreterContext\n",
    "\n",
    "val resourcePool = InterpreterContext.get().getResourcePool()\n",
    "\n",
    "resourcePool.put(\"flinkBeta\", beta.asFormatString)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%sparkMahout\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "z.put(\"sparkBeta\", beta.asFormatString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "\n",
    "import ast\n",
    "\n",
    "flinkBetaDict = ast.literal_eval(z.get(\"flinkBeta\"))\n",
    "sparkBetaDict = ast.literal_eval(z.get(\"sparkBeta\"))\n",
    "\n",
    "print \"----------------- differences between betas calulated in Flink and Spark-----------------\"\n",
    "for i in range(0,4):\n",
    "    print \"beta\", i, \": \" , flinkBetaDict[i] - sparkBetaDict[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Mahout with R\n",
    "\n",
    "The following examples show how we can leverage R to plot our results from Mahout\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%flinkMahout\n",
    "val mxRnd = Matrices.symmetricUniformView(5000, 2, 1234)\n",
    "val drmRand = drmParallelize(mxRnd)\n",
    "\n",
    "\n",
    "val drmSin = drmRand.mapBlock() {case (keys, block) =>  \n",
    "  val blockB = block.like()\n",
    "  for (i <- 0 until block.nrow) {\n",
    "    blockB(i, 0) = block(i, 0) \n",
    "    blockB(i, 1) = Math.sin((block(i, 0) * 8))\n",
    "  }\n",
    "  keys -> blockB\n",
    "}\n",
    "\n",
    "resourcePool.put(\"flinkSinDrm\", drm.drmSampleToTSV(drmSin, 0.85))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%sparkMahout\n",
    "val mxRnd = Matrices.symmetricUniformView(5000, 2, 1234)\n",
    "val drmRand = drmParallelize(mxRnd)\n",
    "\n",
    "\n",
    "val drmSin = drmRand.mapBlock() {case (keys, block) =>  \n",
    "  val blockB = block.like()\n",
    "  for (i <- 0 until block.nrow) {\n",
    "    blockB(i, 0) = block(i, 0) \n",
    "    blockB(i, 1) = Math.sin((block(i, 0) * 8))\n",
    "  }\n",
    "  keys -> blockB\n",
    "}\n",
    "\n",
    "z.put(\"sparkSinDrm\", org.apache.mahout.math.drm.drmSampleToTSV(drmSin, 0.85))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark.r {\"imageWidth\": \"400px\"}\n",
    "\n",
    "library(\"ggplot2\")\n",
    "\n",
    "flinkSinStr = z.get(\"flinkSinDrm\")\n",
    "sparkSinStr = z.get(\"sparkSinDrm\")\n",
    "\n",
    "flinkData <- read.table(text= flinkSinStr, sep=\"\\t\", header=FALSE)\n",
    "sparkData <- read.table(text= sparkSinStr, sep=\"\\t\", header=FALSE)\n",
    "\n",
    "plot(flinkData,  col=\"red\")\n",
    "# Graph trucks with red dashed line and square points\n",
    "points(sparkData, col=\"blue\")\n",
    "\n",
    "# Create a title with a red, bold/italic font\n",
    "title(main=\"Sampled Mahout Sin Graph in R\", col.main=\"black\", font.main=4)\n",
    "\n",
    "legend(\"bottomright\", c(\"Apache Flink\", \"Apache Spark\"), col= c(\"red\", \"blue\"), pch= c(22, 22)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%flinkMahout\n",
    "\n",
    "val mxRnd3d = Matrices.symmetricUniformView(5000, 3, 1234)\n",
    "val drmRand3d = drmParallelize(mxRnd3d)\n",
    "\n",
    "val drmGauss = drmRand3d.mapBlock() {case (keys, block) =>\n",
    "  val blockB = block.like()\n",
    "  for (i <- 0 until block.nrow) {\n",
    "    val x: Double = block(i, 0)\n",
    "    val y: Double = block(i, 1)\n",
    "    val z: Double = block(i, 2)\n",
    "\n",
    "    blockB(i, 0) = x\n",
    "    blockB(i, 1) = y\n",
    "    blockB(i, 2) = Math.exp(-((Math.pow(x, 2)) + (Math.pow(y, 2)))/2)\n",
    "  }\n",
    "  keys -> blockB\n",
    "}\n",
    "\n",
    "resourcePool.put(\"flinkGaussDrm\", drm.drmSampleToTSV(drmGauss, 50.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark.r {\"imageWidth\": \"400px\"}\n",
    "\n",
    "library(scatterplot3d)\n",
    "\n",
    "\n",
    "flinkGaussStr = z.get(\"flinkGaussDrm\")\n",
    "flinkData <- read.table(text= flinkGaussStr, sep=\"\\t\", header=FALSE)\n",
    "\n",
    "scatterplot3d(flinkData, color=\"green\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** To install `scatterplot3d` on Ubuntu use:\n",
    "\n",
    "```sh\n",
    "sudo apt-get install r-cran-scatterplot3d\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2.0.0",
   "language": "python",
   "name": "spark2"
  },
  "language_info": {
   "codemirror_mode": "text/python",
   "file_extension": ".py",
   "mimetype": "text/python",
   "name": "scala",
   "pygments_lexer": "python",
   "version": "3.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
