{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducción a [Apache Spark](http://spark.apache.org/)\n",
    "==========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plataforma de computación cluster rápida\n",
    "\n",
    "-   Extiende modelo MapReduce soportando de manera eficiente otros tipos\n",
    "    de computación\n",
    "\n",
    "    -   queries interactivas\n",
    "\n",
    "    -   procesado streaming\n",
    "\n",
    "-   Soporta computaciones en memoria\n",
    "\n",
    "-   Mejora a MapReduce para aplicaciones complejas (10-20x más rápido)\n",
    "\n",
    "#### Propósito general\n",
    "\n",
    "-   Modos de funcionamiento batch, interactivo o streaming\n",
    "\n",
    "-   Reduce el número de herramientas a emplear y mantener"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Historia\n",
    "\n",
    "-   Iniciado en el 2009 en el UC Berkeley RAD Lab (AMPLab)\n",
    "\n",
    "    -   Motivado por la ineficiencia de MapReduce para trabajos\n",
    "        iterativos e interactivos\n",
    "\n",
    "-   Mayores contribuidores: [Databricks](https://databricks.com/),\n",
    "    Yahoo! e Intel\n",
    "\n",
    "-   Declarado open source en marzo del 2010\n",
    "\n",
    "-   Transferido a la Apache Software Foundation en junio de 2013, TLP en\n",
    "    febrero de 2014\n",
    "\n",
    "-   Uno de los proyectos Big Data más activos\n",
    "\n",
    "-   Versión 1.0 lanzada en mayo de 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Características de Spark\n",
    "\n",
    "-   Soporta gran variedad de workloads: batch, queries interactivas,\n",
    "    streaming, machine learning, procesado de grafos\n",
    "\n",
    "-   APIs en Scala, Java, Python, SQL y R\n",
    "\n",
    "-   Shells interactivos en Scala y Python\n",
    "\n",
    "-   Se integra con otras soluciones BigData: HDFS, Cassandra, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La pila Spark\n",
    "<hr />\n",
    "![sparkstack](http://localhost:8085/figs/sparkstack.png)\n",
    "(Fuente: H. Karau, A. Konwinski, P. Wendell, M. Zaharia, \"Learning Spark\", O'Reilly, 2015)\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conceptos clave\n",
    "<hr />\n",
    "![sparkcontext](http://localhost:8085/figs/sparkcontext.png)\n",
    "(Fuente: H. Karau, A. Konwinski, P. Wendell, M. Zaharia, \"Learning Spark\", O'Reilly, 2015)\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Driver\n",
    "\n",
    "-   Crea un `SparkContext`\n",
    "\n",
    "-   Convierte el programa de usuario en tareas:\n",
    "\n",
    "    -   `DAG` de operaciones lógico -> plan de ejecución físico\n",
    "\n",
    "-   Planifica las tareas en los ejecutores\n",
    "\n",
    "#### SparkContext\n",
    "\n",
    "-   El SparkContext realiza la conexión con el cluster\n",
    "\n",
    "    -   Permite construir RDDs a partir de ficheros, listas u otros\n",
    "        objetos\n",
    "\n",
    "-   En el notebook (o el shell de Spark), se define automáticamente (variable `sc`)\n",
    "\n",
    "-   Creación en un script Python\n",
    "\n",
    "        from pyspark import SparkContext\n",
    "        sc = SparkContext(master=\"local\", appName=\"Mi app\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Creación en un programa Scala\n",
    "   \n",
    "        import org.apache.spark.SparkContext\n",
    "        import org.apache.spark.SparkContext._\n",
    "        import org.apache.spark.SparkConf\n",
    "        val conf = new SparkConf().setMaster(\"local\").setAppName(\"My App\")\n",
    "        val sc = new SparkContext(conf)\n",
    "       \n",
    "\n",
    "#### Executors\n",
    "\n",
    "-   Ejecutan las tareas individuales y devuelven los resultados al\n",
    "    Driver\n",
    "\n",
    "-   Proporcionan almacenamiento en memoria para los datos de las tareas\n",
    "\n",
    "#### Cluster Manager\n",
    "\n",
    "-   Componente *enchufable* en Spark\n",
    "\n",
    "-   YARN, Mesos o Spark Standalone"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2.0.0",
   "language": "python",
   "name": "spark2"
  },
  "language_info": {
   "codemirror_mode": "text/python",
   "file_extension": ".py",
   "mimetype": "text/python",
   "name": "scala",
   "pygments_lexer": "python",
   "version": "3.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
