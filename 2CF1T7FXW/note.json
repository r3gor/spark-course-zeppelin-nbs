{
  "paragraphs": [
    {
      "text": "%md\nLectura y escritura de ficheros\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d",
      "user": "anonymous",
      "dateUpdated": "Jun 4, 2023 7:25:45 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch1\u003eLectura y escritura de ficheros\u003c/h1\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1490983056725_1753162031",
      "id": "20170331-175736_1251486193",
      "dateCreated": "Mar 31, 2017 5:57:36 PM",
      "dateStarted": "Jul 15, 2017 11:37:54 AM",
      "dateFinished": "Jul 15, 2017 11:37:56 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Sistemas de ficheros soportados\n-   Igual que Hadoop, Spark soporta diferentes filesystems: local, HDFS, Amazon S3\n\n    -   En general, soporta cualquier fuente de datos que se pueda leer con Hadoop\n\n-   También, acceso a bases de datos relacionales o noSQL\n\n    -   MySQL, Postgres, etc. mediante JDBC\n    -   Apache Hive, HBase, Cassandra o Elasticsearch\n\n### Formatos de fichero soportados\n\n-   Spark puede acceder a diferentes tipos de ficheros:\n\n    -   Texto plano, CSV, ficheros sequence, JSON, *protocol buffers* y *object files*\n        -   Soporta ficheros comprimidos\n    -   Veremos el acceso a algunos tipos en esta clase, y dejaremos otros para más adelante \n",
      "user": "anonymous",
      "dateUpdated": "Jun 4, 2023 7:25:45 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eSistemas de ficheros soportados\u003c/h3\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    \u003cp\u003eIgual que Hadoop, Spark soporta diferentes filesystems: local, HDFS, Amazon S3\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e\n      \u003cp\u003eEn general, soporta cualquier fuente de datos que se pueda leer con Hadoop\u003c/p\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eTambién, acceso a bases de datos relacionales o noSQL\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eMySQL, Postgres, etc. mediante JDBC\u003c/li\u003e\n      \u003cli\u003eApache Hive, HBase, Cassandra o Elasticsearch\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eFormatos de fichero soportados\u003c/h3\u003e\n\u003cul\u003e\n  \u003cli\u003eSpark puede acceder a diferentes tipos de ficheros:\n    \u003cul\u003e\n      \u003cli\u003eTexto plano, CSV, ficheros sequence, JSON, \u003cem\u003eprotocol buffers\u003c/em\u003e y \u003cem\u003eobject files\u003c/em\u003e\n        \u003cul\u003e\n          \u003cli\u003eSoporta ficheros comprimidos\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003eVeremos el acceso a algunos tipos en esta clase, y dejaremos otros para más adelante\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1500118674272_-801862945",
      "id": "20170715-113754_1944635351",
      "dateCreated": "Jul 15, 2017 11:37:54 AM",
      "dateStarted": "Jul 17, 2017 5:01:41 PM",
      "dateFinished": "Jul 17, 2017 5:01:41 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n#### Ejemplos con ficheros de texto\n\nEn el directorio `../datos/libros` hay un conjunto de ficheros de texto comprimidos.",
      "user": "anonymous",
      "dateUpdated": "Jun 4, 2023 7:25:45 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch4\u003eEjemplos con ficheros de texto\u003c/h4\u003e\n\u003cp\u003eEn el directorio \u003ccode\u003e../datos/libros\u003c/code\u003e hay un conjunto de ficheros de texto comprimidos.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1500118745953_-1002008994",
      "id": "20170715-113905_678503000",
      "dateCreated": "Jul 15, 2017 11:39:05 AM",
      "dateStarted": "Jul 15, 2017 12:26:46 PM",
      "dateFinished": "Jul 15, 2017 12:26:46 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\n# Ficheros de entrada\nls ../datos/libros",
      "user": "anonymous",
      "dateUpdated": "Jun 4, 2023 7:25:45 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "pg14329.txt.gz\npg1619.txt.gz\npg16625.txt.gz\npg17013.txt.gz\npg17073.txt.gz\npg18005.txt.gz\npg2000.txt.gz\npg24536.txt.gz\npg25640.txt.gz\npg25807.txt.gz\npg32315.txt.gz\npg5201.txt.gz\npg7109.txt.gz\npg8870.txt.gz\npg9980.txt.gz\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1500118790611_1294385336",
      "id": "20170715-113950_1612698555",
      "dateCreated": "Jul 15, 2017 11:39:50 AM",
      "dateStarted": "Jun 4, 2023 7:25:45 AM",
      "dateFinished": "Jun 4, 2023 7:25:46 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Funciones de lectura y escritura con ficheros de texto\n\n\n- `sc.textFile(nombrefichero/directorio)` Crea un RDD a partir las líneas de uno o varios ficheros de texto\n    - Si se especifica un directorio, se leen todos los ficheros del mismo, creando una partición por fichero\n    - Los ficheros pueden estar comprimidos, en diferentes formatos (gz, bz2,...)\n    - Pueden especificarse comodines en los nombres de los ficheros\n- `sc.wholeTextFiles(nombrefichero/directorio)` Lee ficheros y devuelve un RDD clave/valor\n    - clave: path completo al fichero\n    - valor: el texto completo del fichero\n- `rdd.saveAsTextFile(directorio_salida)` Almacena el RDD en formato texto en el directorio indicado\n    - Crea un fichero por partición del rdd\n",
      "user": "anonymous",
      "dateUpdated": "Jun 4, 2023 7:25:45 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eFunciones de lectura y escritura con ficheros de texto\u003c/h3\u003e\n\u003cul\u003e\n  \u003cli\u003e\u003ccode\u003esc.textFile(nombrefichero/directorio)\u003c/code\u003e Crea un RDD a partir las líneas de uno o varios ficheros de texto\n    \u003cul\u003e\n      \u003cli\u003eSi se especifica un directorio, se leen todos los ficheros del mismo, creando una partición por fichero\u003c/li\u003e\n      \u003cli\u003eLos ficheros pueden estar comprimidos, en diferentes formatos (gz, bz2,\u0026hellip;)\u003c/li\u003e\n      \u003cli\u003ePueden especificarse comodines en los nombres de los ficheros\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ccode\u003esc.wholeTextFiles(nombrefichero/directorio)\u003c/code\u003e Lee ficheros y devuelve un RDD clave/valor\n    \u003cul\u003e\n      \u003cli\u003eclave: path completo al fichero\u003c/li\u003e\n      \u003cli\u003evalor: el texto completo del fichero\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ccode\u003erdd.saveAsTextFile(directorio_salida)\u003c/code\u003e Almacena el RDD en formato texto en el directorio indicado\n    \u003cul\u003e\n      \u003cli\u003eCrea un fichero por partición del rdd\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1500120467003_287319032",
      "id": "20170715-120747_1605706098",
      "dateCreated": "Jul 15, 2017 12:07:47 PM",
      "dateStarted": "Jul 17, 2017 4:14:15 PM",
      "dateFinished": "Jul 17, 2017 4:14:15 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n# Lee todos los ficheros del directorio\n# y crea un RDD con las líneas\nlineas \u003d sc.textFile(\"../datos/libros\")\n\n# Se crea una partición por fichero de entrada\nprint(\"Número de particiones del RDD lineas \u003d {0}\".format(\n       lineas.getNumPartitions()))",
      "user": "anonymous",
      "dateUpdated": "Jun 4, 2023 7:25:46 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Número de particiones del RDD lineas \u003d 15\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1500118868724_1140039730",
      "id": "20170715-114108_2081955705",
      "dateCreated": "Jul 15, 2017 11:41:08 AM",
      "dateStarted": "Jun 4, 2023 7:25:46 AM",
      "dateFinished": "Jun 4, 2023 7:26:53 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n# Obtén las palabras usando el método split (split usa un espacio como delimitador por defecto)\npalabras \u003d lineas.flatMap(lambda x: x.split())\nprint(\"Número de particiones del RDD palabras \u003d {0}\".format(\n       palabras.getNumPartitions()))",
      "user": "anonymous",
      "dateUpdated": "Jun 4, 2023 7:25:46 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Número de particiones del RDD palabras \u003d 15\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1500310549988_1876915601",
      "id": "20170717-165549_824338572",
      "dateCreated": "Jul 17, 2017 4:55:49 PM",
      "dateStarted": "Jun 4, 2023 7:25:50 AM",
      "dateFinished": "Jun 4, 2023 7:26:53 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n# Reparticiono el RDD en 4 particiones       \npalabras2 \u003d palabras.coalesce(4)\nprint(\"Número de particiones del RDD palabras2 \u003d {0}\".format(\n       palabras2.getNumPartitions()))",
      "user": "anonymous",
      "dateUpdated": "Jun 4, 2023 7:25:49 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Número de particiones del RDD palabras2 \u003d 4\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1500310554533_1271336156",
      "id": "20170717-165554_447956162",
      "dateCreated": "Jul 17, 2017 4:55:54 PM",
      "dateStarted": "Jun 4, 2023 7:26:53 AM",
      "dateFinished": "Jun 4, 2023 7:26:53 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n# Toma una muestra aleatoria de palabras\nprint(palabras2.takeSample(False, 10))",
      "user": "anonymous",
      "dateUpdated": "Jun 4, 2023 7:25:50 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "[u\u0027de\u0027, u\u0027_pues\u0027, u\u0027y\u0027, u\u0027con\u0027, u\u0027penado\u0027, u\u0027dem\\xe1s\u0027, u\u0027pobre\u0027, u\u0027parec\\xedan\u0027, u\u0027debemos\u0027, u\u0027al\u0027]\n"
          }
        ]
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://10.0.2.15:4040/jobs/job?id\u003d0",
            "http://10.0.2.15:4040/jobs/job?id\u003d1"
          ],
          "interpreterSettingId": "2CET3TKHW"
        }
      },
      "apps": [],
      "jobName": "paragraph_1500310556807_-942139590",
      "id": "20170717-165556_1006378560",
      "dateCreated": "Jul 17, 2017 4:55:56 PM",
      "dateStarted": "Jun 4, 2023 7:26:53 AM",
      "dateFinished": "Jun 4, 2023 7:27:05 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n# Salva el RDD palabras en varios ficheros de salida\n# (un fichero por partición)\npalabras2.saveAsTextFile(\"file:///tmp/salidatxt\")",
      "user": "anonymous",
      "dateUpdated": "Jun 4, 2023 7:25:50 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://10.0.2.15:4040/jobs/job?id\u003d2"
          ],
          "interpreterSettingId": "2CET3TKHW"
        }
      },
      "apps": [],
      "jobName": "paragraph_1500310558693_-1472323575",
      "id": "20170717-165558_378255199",
      "dateCreated": "Jul 17, 2017 4:55:58 PM",
      "dateStarted": "Jun 4, 2023 7:26:53 AM",
      "dateFinished": "Jun 4, 2023 7:27:11 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\n# Ficheros de salida\nls -l /tmp/salidatxt\nhead /tmp/salidatxt/part-00002",
      "user": "anonymous",
      "dateUpdated": "Jun 4, 2023 7:25:50 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "ls: cannot access /tmp/salidatxt: No such file or directory\nhead: cannot open \u0027/tmp/salidatxt/part-00002\u0027 for reading: No such file or directory\n"
          },
          {
            "type": "TEXT",
            "data": "ExitValue: 1"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1500119121338_-1509207091",
      "id": "20170715-114521_158888604",
      "dateCreated": "Jul 15, 2017 11:45:21 AM",
      "dateStarted": "Jun 4, 2023 7:25:50 AM",
      "dateFinished": "Jun 4, 2023 7:25:50 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n# Lee los ficheros y devuelve un RDD clave/valor\n# clave-\u003enombre fichero, valor-\u003efichero completo\nprdd \u003d sc.wholeTextFiles(\"../datos/libros/p*.gz\")\nprint(\"Número de particiones del RDD prdd \u003d {0}\\n\".format(\n       prdd.getNumPartitions()))",
      "user": "anonymous",
      "dateUpdated": "Jun 4, 2023 7:25:50 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Número de particiones del RDD prdd \u003d 2\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1500119206726_-79910899",
      "id": "20170715-114646_1618556398",
      "dateCreated": "Jul 15, 2017 11:46:46 AM",
      "dateStarted": "Jun 4, 2023 7:27:05 AM",
      "dateFinished": "Jun 4, 2023 7:27:12 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n# Obtiene un lista clave/valor\n# clave-\u003enombre fichero, valor-\u003enumero de palabras\nlista \u003d prdd.mapValues(lambda x: len(x.split())).collect()\n\nfor libro in lista:\n    print(\"El fichero {0:14s} tiene {1:6d} palabras\".format(\n           libro[0].split(\"/\")[-1], libro[1]))",
      "user": "anonymous",
      "dateUpdated": "Jun 4, 2023 7:25:50 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "El fichero pg1619.txt.gz  tiene 109878 palabras\nEl fichero pg25807.txt.gz tiene  15014 palabras\nEl fichero pg7109.txt.gz  tiene  35037 palabras\nEl fichero pg32315.txt.gz tiene  46142 palabras\nEl fichero pg24536.txt.gz tiene 134016 palabras\nEl fichero pg16625.txt.gz tiene 170900 palabras\nEl fichero pg17013.txt.gz tiene 396086 palabras\nEl fichero pg9980.txt.gz  tiene  34014 palabras\nEl fichero pg18005.txt.gz tiene  86446 palabras\nEl fichero pg2000.txt.gz  tiene 384258 palabras\nEl fichero pg17073.txt.gz tiene 309473 palabras\nEl fichero pg14329.txt.gz tiene 183777 palabras\nEl fichero pg5201.txt.gz  tiene  49441 palabras\nEl fichero pg8870.txt.gz  tiene  54348 palabras\nEl fichero pg25640.txt.gz tiene 207338 palabras\n"
          }
        ]
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://10.0.2.15:4040/jobs/job?id\u003d3"
          ],
          "interpreterSettingId": "2CET3TKHW"
        }
      },
      "apps": [],
      "jobName": "paragraph_1500310633119_240117159",
      "id": "20170717-165713_317028187",
      "dateCreated": "Jul 17, 2017 4:57:13 PM",
      "dateStarted": "Jun 4, 2023 7:27:12 AM",
      "dateFinished": "Jun 4, 2023 7:27:13 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Ficheros Sequence\nFicheros clave/valor usados en Hadoop\n\n-   Sus elementos implementan la interfaz [`Writable`](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/io/Writable.html)",
      "user": "anonymous",
      "dateUpdated": "Jun 4, 2023 7:25:50 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eFicheros Sequence\u003c/h3\u003e\n\u003cp\u003eFicheros clave/valor usados en Hadoop\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eSus elementos implementan la interfaz \u003ca href\u003d\"https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/io/Writable.html\"\u003e\u003ccode\u003eWritable\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1500121763411_710978477",
      "id": "20170715-122923_832495602",
      "dateCreated": "Jul 15, 2017 12:29:23 PM",
      "dateStarted": "Jul 17, 2017 4:38:20 PM",
      "dateFinished": "Jul 17, 2017 4:38:20 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nrdd \u003d sc.parallelize([(\"a\",2), (\"b\",5), (\"a\",8)], 2)\n\n# Salvamos el RDD clave valor como fichero de secuencias\nrdd.saveAsSequenceFile(\"file:///tmp/sequenceoutdir2\")",
      "user": "anonymous",
      "dateUpdated": "Jun 4, 2023 7:25:51 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://10.0.2.15:4040/jobs/job?id\u003d4",
            "http://10.0.2.15:4040/jobs/job?id\u003d5",
            "http://10.0.2.15:4040/jobs/job?id\u003d6"
          ],
          "interpreterSettingId": "2CET3TKHW"
        }
      },
      "apps": [],
      "jobName": "paragraph_1500308028578_1594710243",
      "id": "20170717-161348_41044279",
      "dateCreated": "Jul 17, 2017 4:13:48 PM",
      "dateStarted": "Jun 4, 2023 7:27:12 AM",
      "dateFinished": "Jun 4, 2023 7:27:14 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\necho \u0027Directorio de salida\u0027\nls -l /tmp/sequenceoutdir2\necho \u0027Intenta leer uno de los fichero\u0027\ncat /tmp/sequenceoutdir2/part-00000\necho\necho  \u0027Lee el fichero usando Hadoop\u0027\n/opt/hadoop/bin/hdfs dfs -text /tmp/sequenceoutdir2/part-00001",
      "user": "anonymous",
      "dateUpdated": "Jun 4, 2023 7:25:51 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Directorio de salida\nIntenta leer uno de los fichero\n\nLee el fichero usando Hadoop\nls: cannot access /tmp/sequenceoutdir2: No such file or directory\ncat: /tmp/sequenceoutdir2/part-00000: No such file or directory\ntext: `/tmp/sequenceoutdir2/part-00001\u0027: No such file or directory\n"
          },
          {
            "type": "TEXT",
            "data": "ExitValue: 1"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1500308596977_2135461404",
      "id": "20170717-162316_1566090953",
      "dateCreated": "Jul 17, 2017 4:23:16 PM",
      "dateStarted": "Jun 4, 2023 7:25:51 AM",
      "dateFinished": "Jun 4, 2023 7:26:00 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n# Lo leemos en otro RDD\nrdd2 \u003d sc.sequenceFile(\"file:///tmp/sequenceoutdir2\", \n                       \"org.apache.hadoop.io.Text\", \n                       \"org.apache.hadoop.io.IntWritable\")\n                       \nprint(\"Contenido del RDD {0}\".format(rdd2.collect()))",
      "user": "anonymous",
      "dateUpdated": "Jun 4, 2023 7:25:51 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Contenido del RDD [(u\u0027a\u0027, 2), (u\u0027b\u0027, 5), (u\u0027a\u0027, 8)]\n"
          }
        ]
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://10.0.2.15:4040/jobs/job?id\u003d7",
            "http://10.0.2.15:4040/jobs/job?id\u003d8"
          ],
          "interpreterSettingId": "2CET3TKHW"
        }
      },
      "apps": [],
      "jobName": "paragraph_1500308632369_-1217209863",
      "id": "20170717-162352_788726266",
      "dateCreated": "Jul 17, 2017 4:23:52 PM",
      "dateStarted": "Jun 4, 2023 7:27:13 AM",
      "dateFinished": "Jun 4, 2023 7:27:14 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Formatos de entrada/salida de Hadoop\nSpark puede interactuar con cualquier formato de fichero soportado por Hadoop \n- Soporta las APIs “vieja” y “nueva”\n- Permite acceder a otros tipos de almacenamiento (no fichero), p.e. HBase o MongoDB, a través de `saveAsHadoopDataSet` y/o `saveAsNewAPIHadoopDataSet`\n",
      "user": "anonymous",
      "dateUpdated": "Jun 4, 2023 7:25:51 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eFormatos de entrada/salida de Hadoop\u003c/h3\u003e\n\u003cp\u003eSpark puede interactuar con cualquier formato de fichero soportado por Hadoop\u003cbr/\u003e- Soporta las APIs “vieja” y “nueva”\u003cbr/\u003e- Permite acceder a otros tipos de almacenamiento (no fichero), p.e. HBase o MongoDB, a través de \u003ccode\u003esaveAsHadoopDataSet\u003c/code\u003e y/o \u003ccode\u003esaveAsNewAPIHadoopDataSet\u003c/code\u003e\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1500308263193_54497455",
      "id": "20170717-161743_1876544820",
      "dateCreated": "Jul 17, 2017 4:17:43 PM",
      "dateStarted": "Jul 17, 2017 4:19:37 PM",
      "dateFinished": "Jul 17, 2017 4:19:37 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n# Salvamos el RDD clave/valor como fichero de texto Hadoop (TextOutputFormat)\nrdd.saveAsNewAPIHadoopFile(\"file:///tmp/hadoopfileoutdir\", \n                            \"org.apache.hadoop.mapreduce.lib.output.TextOutputFormat\",\n                            \"org.apache.hadoop.io.Text\",\n                            \"org.apache.hadoop.io.IntWritable\")\n                            ",
      "user": "anonymous",
      "dateUpdated": "Jun 4, 2023 7:25:51 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://10.0.2.15:4040/jobs/job?id\u003d9",
            "http://10.0.2.15:4040/jobs/job?id\u003d10"
          ],
          "interpreterSettingId": "2CET3TKHW"
        }
      },
      "apps": [],
      "jobName": "paragraph_1500308377843_1015351731",
      "id": "20170717-161937_2014632436",
      "dateCreated": "Jul 17, 2017 4:19:37 PM",
      "dateStarted": "Jun 4, 2023 7:27:14 AM",
      "dateFinished": "Jun 4, 2023 7:27:15 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\necho \u0027Directorio de salida\u0027\nls -l /tmp/hadoopfileoutdir\ncat /tmp/hadoopfileoutdir/part-r-00001",
      "user": "anonymous",
      "dateUpdated": "Jun 4, 2023 7:25:52 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "ls: cannot access /tmp/hadoopfileoutdir: No such file or directory\ncat: /tmp/hadoopfileoutdir/part-r-00001: No such file or directory\nDirectorio de salida\n"
          },
          {
            "type": "TEXT",
            "data": "ExitValue: 1"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1500308428501_-1296927861",
      "id": "20170717-162028_1424644460",
      "dateCreated": "Jul 17, 2017 4:20:28 PM",
      "dateStarted": "Jun 4, 2023 7:25:52 AM",
      "dateFinished": "Jun 4, 2023 7:25:52 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n# Lo leemos como fichero clave-valor Hadoop (KeyValueTextInputFormat)\nrdd3 \u003d sc.newAPIHadoopFile(\"file:///tmp/hadoopfileoutdir\", \n                          \"org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat\",\n                          \"org.apache.hadoop.io.Text\",\n                          \"org.apache.hadoop.io.IntWritable\")\n                          \nprint(\"Contenido del RDD {0}\".format(rdd3.collect()))",
      "user": "anonymous",
      "dateUpdated": "Jun 4, 2023 7:25:52 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Contenido del RDD [(u\u0027b\u0027, u\u00275\u0027), (u\u0027a\u0027, u\u00278\u0027), (u\u0027a\u0027, u\u00272\u0027)]\n"
          }
        ]
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://10.0.2.15:4040/jobs/job?id\u003d11",
            "http://10.0.2.15:4040/jobs/job?id\u003d12"
          ],
          "interpreterSettingId": "2CET3TKHW"
        }
      },
      "apps": [],
      "jobName": "paragraph_1500308467782_-449448628",
      "id": "20170717-162107_429179099",
      "dateCreated": "Jul 17, 2017 4:21:07 PM",
      "dateStarted": "Jun 4, 2023 7:27:15 AM",
      "dateFinished": "Jun 4, 2023 7:27:15 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Tarea\n\nA partir del fichero apat63_99.txt, crear un conjunto de ficheros secuencia, que se almacenarán en el directorio apat63_99-seq. En estos ficheros, la clave tiene que ser el país (campo \"COUNTRY\") y el valor un string formado por el número de patente (campo \"PATENT\") y el año de concesión (campo \"GYEAR\") separados por una coma. Una línea de esto ficheros será, por ejemplo:\n\n\u003e BE \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 3070801,1963",
      "user": "anonymous",
      "dateUpdated": "Jun 4, 2023 7:25:52 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eTarea\u003c/h3\u003e\n\u003cp\u003eA partir del fichero apat63_99.txt, crear un conjunto de ficheros secuencia, que se almacenarán en el directorio apat63_99-seq. En estos ficheros, la clave tiene que ser el país (campo \u0026ldquo;COUNTRY\u0026rdquo;) y el valor un string formado por el número de patente (campo \u0026ldquo;PATENT\u0026rdquo;) y el año de concesión (campo \u0026ldquo;GYEAR\u0026rdquo;) separados por una coma. Una línea de esto ficheros será, por ejemplo:\u003c/p\u003e\n\u003cblockquote\u003e\n  \u003cp\u003eBE \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 3070801,1963\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1500308840015_-1854275252",
      "id": "20170717-162720_379645653",
      "dateCreated": "Jul 17, 2017 4:27:20 PM",
      "dateStarted": "Jul 23, 2017 11:46:38 AM",
      "dateFinished": "Jul 23, 2017 11:46:38 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n",
      "user": "anonymous",
      "dateUpdated": "Jun 4, 2023 7:25:52 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1500492710808_-410749749",
      "id": "20170719-193150_192336955",
      "dateCreated": "Jul 19, 2017 7:31:50 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Tarea\n\nEscribir un programa Scala Spark que, a partir de los ficheros cite75_99.txt y apat63_99-seq, obtenga, para cada patente, el país, el año y el número de citas.\n\nUtilizar un *full outer join* para unir, por el campo común (el número de patente) los RDDs asociados a ambos ficheros.\n",
      "user": "anonymous",
      "dateUpdated": "Jun 4, 2023 7:25:53 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eTarea\u003c/h3\u003e\n\u003cp\u003eEscribir un programa Scala Spark que, a partir de los ficheros cite75_99.txt y apat63_99-seq, obtenga, para cada patente, el país, el año y el número de citas.\u003c/p\u003e\n\u003cp\u003eUtilizar un \u003cem\u003efull outer join\u003c/em\u003e para unir, por el campo común (el número de patente) los RDDs asociados a ambos ficheros.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1500808974135_-1497601070",
      "id": "20170723-112254_1990355459",
      "dateCreated": "Jul 23, 2017 11:22:54 AM",
      "dateStarted": "Jul 23, 2017 3:52:09 PM",
      "dateFinished": "Jul 23, 2017 3:52:11 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\n",
      "user": "anonymous",
      "dateUpdated": "Jun 4, 2023 7:25:53 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1500811394520_976160858",
      "id": "20170723-120314_1080217243",
      "dateCreated": "Jul 23, 2017 12:03:14 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Curso Spark/07 - Lectura y escritura de ficheros",
  "id": "2CF1T7FXW",
  "angularObjects": {
    "2CCY33GTB:shared_process": [],
    "2CCQCYKJM:shared_process": [],
    "2CD8VB8N5:shared_process": [],
    "2CEZ3N4ZK:shared_process": [],
    "2CCN184W1:shared_process": [],
    "2CCTDCCB9:shared_process": [],
    "2CBRCMJB7:shared_process": [],
    "2CDTHYD1N:shared_process": [],
    "2CD85MNWZ:shared_process": [],
    "2CEM88R8V:shared_process": [],
    "2CBSSQJJR:shared_process": [],
    "2CBG9JDC9:shared_process": [],
    "2CBJUH5Z5:shared_process": [],
    "2CET3TKHW:shared_process": [],
    "2CF1VUR2D:shared_process": [],
    "2CCZDD8PX:shared_process": [],
    "2CESEPECG:shared_process": [],
    "2CCZGPF6E:shared_process": []
  },
  "config": {},
  "info": {}
}