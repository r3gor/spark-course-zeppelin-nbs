{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDDs con pares clave/valor (aka *Pair RDDs*)\n",
    "\n",
    "-   Tipos de datos muy usados en Big Data (MapReduce)\n",
    "\n",
    "-   Spark dispone de operaciones especiales para su manejo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de *Pair RDDs*\n",
    "Los RDDs clave/valor pueden crearse a partir de una lista de tuplas, a partir de otro RDD o mediante un zip de dos RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   A partir de una lista de tuplas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "prdd = sc.parallelize([('a',2), ('b',5), ('a',3)])\n",
    "print(prdd.collect())\n",
    "\n",
    "prdd = sc.parallelize(zip(['a', 'b', 'c'], range(3)))\n",
    "print(prdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val prdd = sc.parallelize(List(('a',2), ('b',5), ('a',3)))\n",
    "prdd.collect()\n",
    "\n",
    "val prdd = sc.parallelize(List('a', 'b', 'c') zip (1 to 3).toList)\n",
    "prdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   A partir de otro RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Ejemplo usando un fichero\n",
    "# Para cada línea ontenemos una tupla, siendo el primer elemento\n",
    "# la primera palabra de la línes, y el segundo la línea completa\n",
    "linesrdd = sc.textFile(\"../datos/quijote.txt\", use_unicode=False)\n",
    "prdd = linesrdd.map(lambda x: (x.split(\" \")[0], x))\n",
    "\n",
    "print('Par (1ª palabra, línea): {0}\\n'.format(prdd.takeSample(False, 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Usando keyBy(f): Crea tuplas de los elementos del RDD usando f para obtener la clave.\n",
    "nrdd = sc.parallelize(xrange(2,5))\n",
    "prdd = nrdd.keyBy(lambda x: x*x)\n",
    "\n",
    "print(prdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# zipWithIndex(): Zipea el RDD con los índices de sus elementos.\n",
    "rdd = sc.parallelize(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'], 3)\n",
    "prdd = rdd.zipWithIndex()\n",
    "print(rdd.glom().collect())\n",
    "\n",
    "print(prdd.collect())\n",
    "\n",
    "# Este método dispara un Spark job cuando el RDD tiene más de una partición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# zipWithUniqueId(): Zipea el RDD con identificadores únicos (long) para cada elemento.\n",
    "# Los elementos en la partición k-ésima obtienen los ids k, n+k, 2*n+k,... siendo n = nº de particiones\n",
    "# No dispara un trabajo Spark\n",
    "rdd = sc.parallelize(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'], 3)\n",
    "print(\"Particionado del RDD: {0}\".format(rdd.glom().collect()))\n",
    "prdd = rdd.zipWithUniqueId()\n",
    "\n",
    "print(prdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mediante un zip de dos RDDs\n",
    "    - Los RDDs deben tener el mismo número de particiones y el mismo número de elementos en cada partición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize(xrange(0, 5), 2)\n",
    "rdd2 = sc.parallelize(range(1000, 1005), 2)\n",
    "prdd = rdd1.zip(rdd2)\n",
    "\n",
    "print(prdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones sobre un único RDD clave/valor\n",
    "Sobre un único RDD clave/valor podemos efectuar transformaciones de agregación a nivel de clave y transformaciones que afectan a las claves o a los valores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformaciones de agregación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `reduceByKey(func)`/`foldByKey(func)`\n",
    "    -   Devuelven un RDD, agrupando los valores asociados a la misma clave mediante `func`\n",
    "    -   Similares a `reduce` y `fold` sobre RDDs simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "prdd   = sc.parallelize([('a', 2), ('b', 5), ('a', 8), ('b', 6), ('b', 2)]).cache()\n",
    "redrdd = prdd.reduceByKey(add)\n",
    "\n",
    "print(redrdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `groupByKey()` agrupa valores asociados a misma clave\n",
    "    - Operación muy costosa en comunicaciones\n",
    "    - Mejor usar operaciones de reducción\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "grouprdd = prdd.groupByKey()\n",
    "\n",
    "print(grouprdd.collect())\n",
    "print\n",
    "\n",
    "lista = [(k, list(v)) for k, v in grouprdd.collect()]\n",
    "print(lista)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `combineByKey(createCombiner(func1), mergeValue(func2), mergeCombiners(func3))`\n",
    "    - Método general para agregación por clave, similar a `aggregate`\n",
    "    - Especifica tres funciones:\n",
    "\n",
    "     1.  `createCombiner` al recorrer los elementos de cada partición, si nos encontramos una clave nueva se crea un acumulador y se inicializa con `func1`\n",
    "\n",
    "     2.  `mergeValue` mezcla los valores de cada clave en cada partición usando `func2`\n",
    "\n",
    "     3.  `mergeCombiners` mezcla los resultados de las diferentes particiones mediante `func3`\n",
    "\n",
    "- Los valores del RDD de salida pueden tener un tipo diferente al de los valores del RDD de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Para cada clave, obten una tupla que tenga la suma y el número de valores\n",
    "print(prdd.collect())\n",
    "sumCount = prdd.combineByKey(\n",
    "                            (lambda x: (x, 1)),\n",
    "                            (lambda x, y: (x[0]+y, x[1]+1)),\n",
    "                            (lambda x, y: (x[0]+y[0], x[1]+y[1])))\n",
    "\n",
    "print(sumCount.collect())\n",
    "\n",
    "# Con el RDD anterior, obtenemos la media de los valores\n",
    "m = sumCount.mapValues(lambda v: float(v[0])/v[1])\n",
    "print(m.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformaciones sobre claves o valores\n",
    "-   `keys()` devuelve un RDD con las claves\n",
    "-   `values()` devuelve un RDD con los valores\n",
    "-   `sortByKey()` devuelve un RDD clave/valor con las claves ordenadas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "print(\"RDD completo: {0:>46s}\".format(prdd.collect()))\n",
    "print(\"RDD con las claves: {0:>25s}\".format(prdd.keys().collect()))\n",
    "print(\"RDD con los valores: {0:>18}\".format(prdd.values().collect()))\n",
    "print(\"RDD con las claves ordenadas: {0}\".format(prdd.sortByKey().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `mapValues(func)` devuelve un RDD aplicando una función sobre los valores\n",
    "-   `flatMapValues(func)` devuelve un RDD aplicando una función sobre los valores y “aplanando” la salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "mapv = prdd.mapValues(lambda x: (x, 10*x))\n",
    "print(mapv.collect())\n",
    "\n",
    "fmapv = prdd.flatMapValues(lambda x: (x, 10*x))\n",
    "print(fmapv.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones sobre dos RDDs clave/valor\n",
    "Combinan dos RDDs de tipo clave/valor para obtener un tercer RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`join`/`leftOuterJoin`/`rightOuterJoin`/`fullOuterJoin` realizan inner/outer/full joins entre los dos RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([(\"a\", 2), (\"b\", 5), (\"a\", 8)]).cache()\n",
    "rdd2 = sc.parallelize([(\"c\", 7), (\"a\", 1)]).cache()\n",
    "\n",
    "rdd3 = rdd1.join(rdd2)\n",
    "\n",
    "print(rdd3.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rdd3 = rdd1.leftOuterJoin(rdd2)\n",
    "\n",
    "print(rdd3.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rdd3 = rdd1.rightOuterJoin(rdd2)\n",
    "\n",
    "print(rdd3.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rdd3 = rdd1.fullOuterJoin(rdd2)\n",
    "\n",
    "print(rdd3.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `subtractByKey` elimina elementos con una clave presente en otro RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rdd3 = rdd1.subtractByKey(rdd2)\n",
    "\n",
    "print(rdd3.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `cogroup` agrupa los datos que comparten la misma clave en ambos RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rdd3 = rdd1.cogroup(rdd2)\n",
    "\n",
    "print(rdd3.collect())\n",
    "\n",
    "map = rdd3.mapValues(lambda v: [list(l) for l in v]).collectAsMap()\n",
    "\n",
    "print(map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acciones sobre RDDs clave/valor\n",
    "Sobre los RDDs clave/valor podemos aplicar las acciones para RDDs simples y algunas adicionales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `collectAsMap()` obtiene el RDD en forma de mapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "prdd = sc.parallelize([(\"a\", 7), (\"b\", 5), (\"a\", 8)]).cache()\n",
    "\n",
    "rddMap = prdd.collectAsMap()\n",
    "\n",
    "print(rddMap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `countByKey()` devuelve un mapa indicando el número de ocurrencias de cada clave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "countMap = prdd.countByKey()\n",
    "\n",
    "print(countMap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `lookup(key)` devuelve una lista con los valores asociados con una clave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "listA = prdd.lookup('a')\n",
    "\n",
    "print(listA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarea - Número de citas de patentes\n",
    "\n",
    "Escribir un programa PySpark que, a partir del fichero apat63_99.txt, obtenga, para cada país de invención (campo \"COUNTRY\" del fichero), el número medio de reivindicaciones (campo \"CLAIMS\" del fichero) de sus patentes.\n",
    "\n",
    "Preguntas de esta tarea\n",
    "- ¿Cuál es el número medio de claims para España (código de país \"ES\")?\n",
    "- ¿Cuál es el número medio de claims para Argentina (código de país \"AR\")?\n",
    "- ¿Cuál es el número medio de claims para México (código de país \"MX\")?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "patentes=sc.textFile(\"../datos/cite75_99.txt\")\n",
    "patentes.take(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TAREA: Obtener las citas recibidas\n",
    "\n",
    "Escribir un programa PySpark que, a partir del fichero cite75_99.txt, obtenga el número de citas que recibe cada patente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TAREA: Número medio de reivindicaciones por país.\n",
    "\n",
    "Escribir un programa PySpark que, a partir del fichero apat63_99.txt, obtenga, para cada país de invención, el número medio de reivindicaciones de sus patentes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2.0.0",
   "language": "python",
   "name": "spark2"
  },
  "language_info": {
   "codemirror_mode": "text/python",
   "file_extension": ".py",
   "mimetype": "text/python",
   "name": "scala",
   "pygments_lexer": "python",
   "version": "3.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
