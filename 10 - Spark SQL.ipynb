{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceptos básicos de Spark SQL\n",
    "-   Interfaz para trabajar con datos estructurados y semiestructurados\n",
    "\n",
    "-   Capacidades principales\n",
    "    -   Lee datos de una gran variedad de fuentes: RDDs, ficheros JSON, Hive, HDFS, Parquet…\n",
    "    -   Permite consultas SQL, tanto desde programas Spark como externas usando conectores estándar (JDBC/ODBC)\n",
    "    -   Integra SQL y código Spark normal (en Python/Java/Scala)\n",
    "\n",
    "-   Contexto SQLContext: punto de entrada (equivalente al SparkContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elemento básico: DataFrames\n",
    "-   Colección distribuida de datos organizada en columnas con nombre\n",
    "    - Conceptualmente equivalente a una tabla en una BD o a un dataframe en R o Python Pandas\n",
    "    - Al igual que los RDDs son inmutables y lazy\n",
    "    - Desarrollados dentro de Spark SQL\n",
    "        - Permite acceder a los datos mediante consultas SQL\n",
    "        - Sustitutos de los RDDs en general\n",
    "\n",
    "-   `DataSet`: nuevo tipo de datos añadido en Spark 1.6\n",
    "    -   Intenta proporcionar los beneficios de los RDDs con las optimizaciones que proporciona el motor de ejecución [Tungsten](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html \"Project Tungsten: Bringing Apache Spark Closer to Bare Metal\") de Spark SQL.\n",
    "    -   Sólo disponible en Scala y Java\n",
    "    -   En [Java](http://spark.apache.org/docs/latest/api/java/index.html \"Interface Row\") y [Scala](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Row \"trait Row extends Serializable\"), un DataFrame es un DataSet de objetos de tipo Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mejora de rendimiento\n",
    "- Spark SQL con DataFrames y DataSets se aprovecha del uso de datos con estructura para optimizar el rendimiento utilizando el optimizador de consultas [Catalyst](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html \"Deep Dive into Spark SQL’s Catalyst Optimizer\")  y el motor de ejecución [Tungsten](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html \"Project Tungsten: Bringing Apache Spark Closer to Bare Metal\").\n",
    "\n",
    "<img src=\"http://localhost:8085/figs/performance.png\" alt=\"Mejora de rendimiento\" style=\"width: 650px;\"/>\n",
    "\n",
    "Fuente: [Recent performance improvements in Apache Spark: SQL, Python, DataFrames, and More](https://databricks.com/blog/2015/04/24/recent-performance-improvements-in-apache-spark-sql-python-dataframes-and-more.html \"Recent performance improvements in Apache Spark: SQL, Python, DataFrames, and More\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de DataFrames\n",
    "Varias formas:\n",
    "\n",
    "- A partir de un RDD de listas/tuplas\n",
    "- A partir de un RDD de objetos Row\n",
    "- A partir de ficheros JSON\n",
    "- A partir de otros almacenamientos (Parquet, Hive,...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame a partir de un RDD de listas/tuplas\n",
    "A partir de un fichero, se crea un RDD de listas que se convierte en un DataFrame. \n",
    "\n",
    "La creación del DataFrame se puede hacer de varias formas:\n",
    "\n",
    "- Infiriendo el esquema\n",
    "- Indicando el esquema de forma explícita"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame a partir de un RDD de listas/tuplas infiriendo el esquema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Leemos el fichero apat63_99.txt.\n",
    "rdd = sc.textFile(\"../datos/apat63_99.txt\").cache()\n",
    "\n",
    "# Le quitamos la cabecera y lo convertimos en un RDD de listas\n",
    "rddSplit = rdd.filter(lambda l: not l.startswith('\"PATENT\"'))\\\n",
    "              .map(lambda l: l.split(\",\")[0:16])\n",
    "\n",
    "# Obtengo la cabecera como una lista de nombres (sin comillas dobles)\n",
    "cabecera = [c.strip('\"') for c in rdd.take(1)[0].split(\",\")[0:16]]\n",
    "\n",
    "print(cabecera)\n",
    "\n",
    "rdd.unpersist()\n",
    "rddSplit.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Dos formas de crear el DataFrame\n",
    "# 1. A partir del método createDataFrame de sqlContext\n",
    "dfInfer1 = sqlContext.createDataFrame(rddSplit, cabecera)\n",
    "\n",
    "# 2. A partir del método toDF del RDD\n",
    "dfInfer2 = rddSplit.toDF(cabecera)\n",
    "\n",
    "dfInfer1.show(10)\n",
    "\n",
    "dfInfer2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Esquema de la tabla\n",
    "dfInfer2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los tipos de datos no se han inferido de forma correcta\n",
    "\n",
    "- Para que los tipos se infieran correctamente, podemos partir de un RDD de listas con los tipos correctos para cada campo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Convierto cambio el tipo de los datos del RDD de listas\n",
    "def toIntSafe(inval):\n",
    "  try:\n",
    "    return int(inval)\n",
    "  except ValueError:\n",
    "    return 0\n",
    "    \n",
    "def toFloatSafe(inval):\n",
    "  try:\n",
    "    return float(inval)\n",
    "  except ValueError:\n",
    "    return 0.0\n",
    "\n",
    "# Dejo todos los campos como Strings, menos el campo 8 (CLAIMS) que lo pongo como entero\n",
    "# y el campo 15 (GENERAL) que lo pongo como float\n",
    "rddTipos = rddSplit.map(lambda l: (l[0], \n",
    "                                   l[1],\n",
    "                                   l[2], \n",
    "                                   l[3], \n",
    "                                   l[4].strip('\"'), \n",
    "                                   l[5].strip('\"'), \n",
    "                                   l[6], \n",
    "                                   l[7], \n",
    "                                   toIntSafe(l[8]),\n",
    "                                   l[9],\n",
    "                                   l[10], \n",
    "                                   l[11], \n",
    "                                   l[12], \n",
    "                                   l[13], \n",
    "                                   l[14], \n",
    "                                   toFloatSafe(l[15])))\n",
    "rddTipos.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "dfInfer3 = sqlContext.createDataFrame(rddTipos, cabecera)\n",
    "\n",
    "dfInfer3.printSchema()\n",
    "\n",
    "dfInfer3.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame a partir de un RDD de listas/tuplas indicando el esquema de forma explícita\n",
    "\n",
    "- Defino el esquema para los elementos de la tabla usando un StructType de StructField\n",
    "    -  StructType: Permite definir un esquema para el DataFrame a partir de una lista de StructFields\n",
    "    -  StructField: Definen el nombre y tipo de cada columna, así como si es nullable o no\n",
    "\n",
    "Tipos definidos en <https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#module-pyspark.sql.types>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Defino el esquema para los elementos de la tabla\n",
    "# StructType -> Permite definir un esquema para el DF a partir de una lista de StructFields\n",
    "# StructField -> Definen el nombre y tipo de cada columna, así como si es nullable o no (campo True)\n",
    "postSchema = StructType([\n",
    "  StructField(cabecera[0], StringType(), False),\n",
    "  StructField(cabecera[1], StringType(), True),\n",
    "  StructField(cabecera[2], StringType(), True),\n",
    "  StructField(cabecera[3], StringType(), True),\n",
    "  StructField(cabecera[4], StringType(), True),\n",
    "  StructField(cabecera[5], StringType(), True),\n",
    "  StructField(cabecera[6], StringType(), True),\n",
    "  StructField(cabecera[7], StringType(), True),\n",
    "  StructField(cabecera[8], IntegerType(), True),\n",
    "  StructField(cabecera[9], StringType(), True),\n",
    "  StructField(cabecera[10], StringType(), True),\n",
    "  StructField(cabecera[11], StringType(), True),\n",
    "  StructField(cabecera[12], StringType(), False),\n",
    "  StructField(cabecera[13], StringType(), True),\n",
    "  StructField(cabecera[14], StringType(), True),\n",
    "  StructField(cabecera[15], FloatType(), True)\n",
    "  ])\n",
    "\n",
    "# Creo el DataFrame\n",
    "dfSchema = sqlContext.createDataFrame(rddTipos, postSchema).cache()\n",
    "\n",
    "rddTipos.unpersist()\n",
    "\n",
    "dfSchema.printSchema()\n",
    "\n",
    "dfSchema.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame a partir de un RDD de objetos Row\n",
    "\n",
    "- [Row](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.Row \"Objeto Row\") Representa una fila de datos en un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Convierto el RDD de listas en un RDD de objetos Row\n",
    "rddRows = rddSplit.map(lambda l: Row(Patent = l[0], \n",
    "                                     Gyear = l[1], \n",
    "                                     Gdate = l[2], \n",
    "                                     Appyear = l[3],\n",
    "                                     Country = l[4],\n",
    "                                     Postate = l[5],\n",
    "                                     Assignee = l[6], \n",
    "                                     Asscode = l[7],\n",
    "                                     Claims = toIntSafe(l[8]),\n",
    "                                     Nclass = l[9], \n",
    "                                     Cat = l[10], \n",
    "                                     Subcat = l[11], \n",
    "                                     Cmade = l[12],\n",
    "                                     Creceive = l[13],\n",
    "                                     Ratiocit = l[14],\n",
    "                                     General = toFloatSafe(l[15])))\n",
    "\n",
    "# El esquema se infiere de los tipos\n",
    "dfRows = sqlContext.createDataFrame(rddRows)\n",
    "\n",
    "print(\"Esquema de la tabla en árbol\")\n",
    "dfRows.printSchema()\n",
    "\n",
    "print(\"Nombres de las columnas\\n{0}\\n\".\n",
    "      format(dfRows.columns))\n",
    "\n",
    "print(\"Tipos de las columnas\\n{0}\\n\".\n",
    "      format(dfRows.dtypes))\n",
    "      \n",
    "rddSplit.unpersist()\n",
    "\n",
    "dfRows.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion de un DataFrame en un RDD de objetos Row\n",
    "\n",
    "- Podemos convertir un DataFrame en un RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rddRows2 = dfSchema.rdd\n",
    "\n",
    "print(\"Muestra un elemento del nuevo RDD\")\n",
    "print(rddRows2.take(1))\n",
    "\n",
    "print(\"Aplicamos un map al RDD\")\n",
    "print(rddRows2.map(lambda r: (r.COUNTRY, r.PATENT)).take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame a partir de un fichero JSON"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%sh\n",
    "cat ../datos/gente.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "dfJson = sqlContext.read.json(\"../datos/gente.json\")\n",
    "\n",
    "dfJson.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardar el DataFrame como fichero JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "dfSchema.write.json(\"/tmp/apat63_99-json\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%sh\n",
    "ls -l /tmp/apat63_99-json"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%sh\n",
    "head -n 10 /tmp/apat63_99-json/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones básicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección y eliminación de columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "dfParcial = dfSchema.select(\"PATENT\", \"GYEAR\", \"COUNTRY\", \"CLAIMS\")\n",
    "dfParcial.show(10)\n",
    "\n",
    "print(\"El objeto dfParcial es de tipo {0}\".format(type(dfParcial)))\n",
    "dfSchema.unpersist()\n",
    "dfParcial.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# También es posible crear objetos de tipo Column\n",
    "colPatent = dfParcial[\"PATENT\"]\n",
    "colCountry = dfParcial.COUNTRY\n",
    "print(\"El objeto colPatent es de tipo {0}\".format(type(colPatent)))\n",
    "print(\"El objeto colCountry es de tipo {0}\".format(type(colCountry)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Y crear un DataFrame a partir de objetos Column, renombrando columnas\n",
    "dfParcial2 = dfParcial.select(colPatent.alias(\"Patente\"), colCountry.alias(\"País\"), dfParcial.GYEAR.alias(\"Año\"))\n",
    "dfParcial2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Se pueden eliminar columnas\n",
    "dfParcial3 = dfParcial.drop(\"CLAIMS\")\n",
    "dfParcial3.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Patentes con CLAIMS > 0\n",
    "dfClaims = dfParcial.where('CLAIMS > 0')\n",
    "print(\"Número de patentes con reivindicaciones: {0}\\n\".\\\n",
    "       format(dfClaims.count()))\n",
    "dfClaims.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Patentes con inventor español\n",
    "dfEsp = dfParcial.filter(colCountry.like('ES'))\n",
    "print(\"Número de patentes con inventor español: {0}\\n\".\\\n",
    "       format(dfEsp.count()))\n",
    "dfEsp.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordenación y agrupamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "dfParcial.orderBy('CLAIMS', ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "grupoPorPais = dfParcial.groupBy('COUNTRY')\n",
    "print(type(grupoPorPais))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "print(\"Número de patentes por país\")\n",
    "grupoPorPais.count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "print(\"Media de reivindicaciones por país\")\n",
    "grupoPorPais.avg('CLAIMS').orderBy('COUNTRY').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "dfEsp80 = dfEsp.where('int(GYEAR) > 1979 and int(GYEAR) < 1990')\n",
    "\n",
    "dfPatYear = dfEsp80.select(dfEsp80.PATENT.alias(\"Patente\"), dfEsp80[\"GYEAR\"].alias(\"Año\"))\n",
    "dfPatYear.show(5)\n",
    "dfPatCountry = dfEsp80.select(dfEsp80.COUNTRY.alias(\"País\"), dfEsp80.PATENT.alias(\"Patente\"))\n",
    "dfPatCountry.show(5)\n",
    "\n",
    "dfPatYear.join(dfPatCountry, \"Patente\", \"inner\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones escalares y agregados\n",
    "\n",
    "Spark ofrece un ámplio abanico de funciones para operar con los DataFrames:\n",
    "\n",
    "- Funciones matemáticas: abs, log, hypot, etc.\n",
    "- Operaciones con strings: lenght, concat, etc.\n",
    "- Operaciones con fechas: year, date_add, etc.\n",
    "- Operaciones de agregación: min, max, count, avg, sum, sumDistinct, stddev, variance, kurtosis, skewness, first, last, etc.\n",
    "\n",
    "Una descripción de estas funciones se puede encontrar en <http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Obtener el máximo, mínimo, media y desviación estándard de las reivindicaciones de las patentes españolas\n",
    "from pyspark.sql.functions import *\n",
    "dfEsp.select(max(\"CLAIMS\"), min(\"CLAIMS\"),avg(\"CLAIMS\"),stddev(\"CLAIMS\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Otra forma de hacer lo mismo\n",
    "dfEsp.describe(\"CLAIMS\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consultas SQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Registra la tabla para usar SQL\n",
    "dfParcial.registerTempTable(\"patentinfo\")\n",
    "sqlContext.sql(\"SELECT COUNTRY,CLAIMS FROM patentinfo WHERE CLAIMS >= 100\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDFs: Funciones definidas por el usuario\n",
    "Si queremos una función que no está implementada, podemos crear nuestra propia función que opera sobre columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BooleanType\n",
    "esPar = udf(lambda n: not n%2, BooleanType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "print(\"Información sobre si el número de reivindicaciones es par o impar.\")\n",
    "dfParcial.select(dfParcial.PATENT, dfParcial.CLAIMS, esPar(dfParcial.CLAIMS).alias(\"Par?\")).orderBy(dfParcial.CLAIMS, ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarea\n",
    "A partir del fichero `cite75_99.txt`vuelve a obtener el número de citas por patente pero usando un DataFrame. Obtén las tres patente que más veces han sido citadas, y el máximo, mínimo y media del número de citas de todas las patentes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2.0.0",
   "language": "python",
   "name": "spark2"
  },
  "language_info": {
   "codemirror_mode": "text/python",
   "file_extension": ".py",
   "mimetype": "text/python",
   "name": "scala",
   "pygments_lexer": "python",
   "version": "3.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
