{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lectura y escritura de ficheros\n",
    "==============================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sistemas de ficheros soportados\n",
    "-   Igual que Hadoop, Spark soporta diferentes filesystems: local, HDFS, Amazon S3\n",
    "\n",
    "    -   En general, soporta cualquier fuente de datos que se pueda leer con Hadoop\n",
    "\n",
    "-   También, acceso a bases de datos relacionales o noSQL\n",
    "\n",
    "    -   MySQL, Postgres, etc. mediante JDBC\n",
    "    -   Apache Hive, HBase, Cassandra o Elasticsearch\n",
    "\n",
    "### Formatos de fichero soportados\n",
    "\n",
    "-   Spark puede acceder a diferentes tipos de ficheros:\n",
    "\n",
    "    -   Texto plano, CSV, ficheros sequence, JSON, *protocol buffers* y *object files*\n",
    "        -   Soporta ficheros comprimidos\n",
    "    -   Veremos el acceso a algunos tipos en esta clase, y dejaremos otros para más adelante \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplos con ficheros de texto\n",
    "\n",
    "En el directorio `../datos/libros` hay un conjunto de ficheros de texto comprimidos."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%sh\n",
    "# Ficheros de entrada\n",
    "ls ../datos/libros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones de lectura y escritura con ficheros de texto\n",
    "\n",
    "\n",
    "- `sc.textFile(nombrefichero/directorio)` Crea un RDD a partir las líneas de uno o varios ficheros de texto\n",
    "    - Si se especifica un directorio, se leen todos los ficheros del mismo, creando una partición por fichero\n",
    "    - Los ficheros pueden estar comprimidos, en diferentes formatos (gz, bz2,...)\n",
    "    - Pueden especificarse comodines en los nombres de los ficheros\n",
    "- `sc.wholeTextFiles(nombrefichero/directorio)` Lee ficheros y devuelve un RDD clave/valor\n",
    "    - clave: path completo al fichero\n",
    "    - valor: el texto completo del fichero\n",
    "- `rdd.saveAsTextFile(directorio_salida)` Almacena el RDD en formato texto en el directorio indicado\n",
    "    - Crea un fichero por partición del rdd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Lee todos los ficheros del directorio\n",
    "# y crea un RDD con las líneas\n",
    "lineas = sc.textFile(\"../datos/libros\")\n",
    "\n",
    "# Se crea una partición por fichero de entrada\n",
    "print(\"Número de particiones del RDD lineas = {0}\".format(\n",
    "       lineas.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Obtén las palabras usando el método split (split usa un espacio como delimitador por defecto)\n",
    "palabras = lineas.flatMap(lambda x: x.split())\n",
    "print(\"Número de particiones del RDD palabras = {0}\".format(\n",
    "       palabras.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Reparticiono el RDD en 4 particiones       \n",
    "palabras2 = palabras.coalesce(4)\n",
    "print(\"Número de particiones del RDD palabras2 = {0}\".format(\n",
    "       palabras2.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Toma una muestra aleatoria de palabras\n",
    "print(palabras2.takeSample(False, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Salva el RDD palabras en varios ficheros de salida\n",
    "# (un fichero por partición)\n",
    "palabras2.saveAsTextFile(\"file:///tmp/salidatxt\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%sh\n",
    "# Ficheros de salida\n",
    "ls -l /tmp/salidatxt\n",
    "head /tmp/salidatxt/part-00002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Lee los ficheros y devuelve un RDD clave/valor\n",
    "# clave->nombre fichero, valor->fichero completo\n",
    "prdd = sc.wholeTextFiles(\"../datos/libros/p*.gz\")\n",
    "print(\"Número de particiones del RDD prdd = {0}\\n\".format(\n",
    "       prdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Obtiene un lista clave/valor\n",
    "# clave->nombre fichero, valor->numero de palabras\n",
    "lista = prdd.mapValues(lambda x: len(x.split())).collect()\n",
    "\n",
    "for libro in lista:\n",
    "    print(\"El fichero {0:14s} tiene {1:6d} palabras\".format(\n",
    "           libro[0].split(\"/\")[-1], libro[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ficheros Sequence\n",
    "Ficheros clave/valor usados en Hadoop\n",
    "\n",
    "-   Sus elementos implementan la interfaz [`Writable`](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/io/Writable.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"a\",2), (\"b\",5), (\"a\",8)], 2)\n",
    "\n",
    "# Salvamos el RDD clave valor como fichero de secuencias\n",
    "rdd.saveAsSequenceFile(\"file:///tmp/sequenceoutdir2\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%sh\n",
    "echo 'Directorio de salida'\n",
    "ls -l /tmp/sequenceoutdir2\n",
    "echo 'Intenta leer uno de los fichero'\n",
    "cat /tmp/sequenceoutdir2/part-00000\n",
    "echo\n",
    "echo  'Lee el fichero usando Hadoop'\n",
    "/opt/hadoop/bin/hdfs dfs -text /tmp/sequenceoutdir2/part-00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Lo leemos en otro RDD\n",
    "rdd2 = sc.sequenceFile(\"file:///tmp/sequenceoutdir2\", \n",
    "                       \"org.apache.hadoop.io.Text\", \n",
    "                       \"org.apache.hadoop.io.IntWritable\")\n",
    "                       \n",
    "print(\"Contenido del RDD {0}\".format(rdd2.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatos de entrada/salida de Hadoop\n",
    "Spark puede interactuar con cualquier formato de fichero soportado por Hadoop \n",
    "- Soporta las APIs “vieja” y “nueva”\n",
    "- Permite acceder a otros tipos de almacenamiento (no fichero), p.e. HBase o MongoDB, a través de `saveAsHadoopDataSet` y/o `saveAsNewAPIHadoopDataSet`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Salvamos el RDD clave/valor como fichero de texto Hadoop (TextOutputFormat)\n",
    "rdd.saveAsNewAPIHadoopFile(\"file:///tmp/hadoopfileoutdir\", \n",
    "                            \"org.apache.hadoop.mapreduce.lib.output.TextOutputFormat\",\n",
    "                            \"org.apache.hadoop.io.Text\",\n",
    "                            \"org.apache.hadoop.io.IntWritable\")\n",
    "                            "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%sh\n",
    "echo 'Directorio de salida'\n",
    "ls -l /tmp/hadoopfileoutdir\n",
    "cat /tmp/hadoopfileoutdir/part-r-00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Lo leemos como fichero clave-valor Hadoop (KeyValueTextInputFormat)\n",
    "rdd3 = sc.newAPIHadoopFile(\"file:///tmp/hadoopfileoutdir\", \n",
    "                          \"org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat\",\n",
    "                          \"org.apache.hadoop.io.Text\",\n",
    "                          \"org.apache.hadoop.io.IntWritable\")\n",
    "                          \n",
    "print(\"Contenido del RDD {0}\".format(rdd3.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea\n",
    "\n",
    "A partir del fichero apat63_99.txt, crear un conjunto de ficheros secuencia, que se almacenarán en el directorio apat63_99-seq. En estos ficheros, la clave tiene que ser el país (campo \"COUNTRY\") y el valor un string formado por el número de patente (campo \"PATENT\") y el año de concesión (campo \"GYEAR\") separados por una coma. Una línea de esto ficheros será, por ejemplo:\n",
    "\n",
    "> BE &nbsp;&nbsp;&nbsp;&nbsp; 3070801,1963"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea\n",
    "\n",
    "Escribir un programa Scala Spark que, a partir de los ficheros cite75_99.txt y apat63_99-seq, obtenga, para cada patente, el país, el año y el número de citas.\n",
    "\n",
    "Utilizar un *full outer join* para unir, por el campo común (el número de patente) los RDDs asociados a ambos ficheros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2.0.0",
   "language": "python",
   "name": "spark2"
  },
  "language_info": {
   "codemirror_mode": "text/python",
   "file_extension": ".py",
   "mimetype": "text/python",
   "name": "scala",
   "pygments_lexer": "python",
   "version": "3.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
