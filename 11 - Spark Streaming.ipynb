{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Streaming\n",
    "\n",
    "-   Procesamiento escalable, *high-throughput* y tolerante a fallos de flujos de datos\n",
    "\n",
    "<img src=\"http://localhost:8085/figs/streaming-flow.png\" alt=\"Flujo de Spark Streaming\" style=\"width: 800px;\"/>\n",
    "\n",
    "-   Entrada desde muchas fuentes: Kafka, Flume, Twitter, ZeroMQ, Kinesis o sockets TCP\n",
    "\n",
    "### Arquitectura de Spark Streaming\n",
    "\n",
    "Abstracción principal: DStream (`discretized stream`).\n",
    "\n",
    "-   Representa un flujo continuo de datos\n",
    "\n",
    "![dstreams](http://localhost:8085/figs/dstreams.png)\n",
    "\n",
    "Arquitectura *micro-batch*\n",
    "\n",
    "-   Los datos recibidos se agrupan en batches\n",
    "-   Los batches se crean a intervalos regulares (batch interval)\n",
    "-   Cada batch forma un RDD, que es procesado por Spark\n",
    "-   Adicionalmente: transformaciones con estado mediante\n",
    "    -   Operaciones con ventanas\n",
    "    -   Tracking del estado por cada clave\n",
    "\n",
    "Página de Spark Streaming: <https://spark.apache.org/streaming/>\n",
    "Documentación principal (de la última versión): <https://spark.apache.org/docs/latest/streaming-programming-guide.html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Streaming: ejemplo de WordCount en red y sin estado\n",
    "\n",
    "Para ejecutar el ejemplo:\n",
    "\n",
    "   1. Ve al terminal desde el que iniciaste la máquina virtual con `vagrant up` y ejecuta la orden `vagrant ssh`\n",
    "   2. Una vez en el terminal de la máquina virtual, usa netcat como un servidor en el puerto 9999\n",
    "\n",
    "    `$ nc -lk 9999`\n",
    "\n",
    "   2. Ejecuta el código PySpark que viene a continuación \n",
    "\n",
    "   3. Escribe líneas en el terminal del netcat, que serán recogidas y procesadas por el script\n",
    "    - Escribe palabras repetidas, para comprobar que las cuenta bien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from operator import add\n",
    "\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "# Contexto Streaming con un batch interval de 5 s\n",
    "ssc = StreamingContext(sc, 5)\n",
    "\n",
    "# DStream que conecta a localhost:9999\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "# Ejecuta un WordCount\n",
    "counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "              .map(lambda word: (word, 1))\\\n",
    "              .reduceByKey(add)\n",
    "              \n",
    "counts.pprint()\n",
    "\n",
    "ssc.start() # Inicia la computacion\n",
    "ssc.awaitTerminationOrTimeout(60) # Espera a que termine (acaba en 60 segundos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Streaming: ejemplo de WordCount en red con estado\n",
    "\n",
    "Repite los pasos anteriores, ejecutando el siguiente código\n",
    "\n",
    " - Comprueba que el número de palabras se acumula entre accesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from operator import add\n",
    "\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "# Contexto Streaming con un batch interval de 5 s\n",
    "ssc = StreamingContext(sc, 5)\n",
    "\n",
    "# DStream que conecta a localhost:9999\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "ssc.checkpoint(\"/tmp/cpdir\") # Activa checkpoint\n",
    "\n",
    "def updateFunc(new_values, last_sum):\n",
    "    return sum(new_values) + (last_sum or 0)\n",
    "\n",
    "counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "              .map(lambda word: (word, 1))\\\n",
    "              .updateStateByKey(updateFunc)\n",
    "\n",
    "counts.pprint()\n",
    "\n",
    "ssc.start() # Inicia la computacion\n",
    "ssc.awaitTerminationOrTimeout(60) # Espera a que termine (acaba en 60 segundos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2.0.0",
   "language": "python",
   "name": "spark2"
  },
  "language_info": {
   "codemirror_mode": "text/python",
   "file_extension": ".py",
   "mimetype": "text/python",
   "name": "scala",
   "pygments_lexer": "python",
   "version": "3.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
