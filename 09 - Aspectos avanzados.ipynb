{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabajos, etapas y tareas\n",
    "-   Un programa Spark define un DAG conectando los diferentes RDDs\n",
    "    -   Las transformaciones crean RDDs hijos a partir de RDDs padres\n",
    "\n",
    "-   Las acciones traducen el DAG en un plan de ejecución\n",
    "    -   El driver envía un *trabajo* (job) para computar todos los RDDs implicados en la acción\n",
    "    -   El job se descompone en una o más *etapas* (stages)\n",
    "    -   Cada etapa está asociada a uno o más RDDs del DAG\n",
    "    -   Las etapas se procesan en orden, lanzándose *tareas* (tasks) individuales que computan segmentos de los RDDs\n",
    "\n",
    "-   Pipelining: varios RDDs se pueden computan en una misma etapa si se verifica que:\n",
    "    -   Los RDDs se pueden obtener de sus padres sin movimiento de datos (p.e. *map*), o bien\n",
    "    -   si alguno de los RDDs se ha *cacheado* en memoria o disco\n",
    "\n",
    "-   En el [interfaz web de Spark](http://localhost:4040 \"PySpark en localhost\") se muestran información sobre las etapas y tareas (más info: método `toDebugString()` de los RDDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "// A partir de los ficheros secuencia de apat63_99-seq obtener para cada país y año el número de patentes\n",
    "import org.apache.hadoop.io.Text\n",
    "\n",
    "val prdd = sc.sequenceFile(\"../datos/apat63_99-seq\", classOf[Text], classOf[Text])\n",
    "println(\"Número de particiones del RDD\"+ prdd.getNumPartitions)\n",
    "\n",
    "//Cada registro de apar63_99-seq tiene un par (país, patente,año)\n",
    "val prdd2 = prdd.map(p => (p._1+\"-\"+p._2.toString().split(\",\")(1), 1) )\n",
    "                .reduceByKey(_+_)\n",
    "                \n",
    "val s = prdd2.take(10)\n",
    "\n",
    "println(\"\\nInformación de depurado:\")\n",
    "println(prdd2.toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acumuladores\n",
    "\n",
    "Permiten agregar valores desde los *worker nodes*, que se pasan al *driver*\n",
    "\n",
    "-   Útiles para contar eventos\n",
    "\n",
    "-   Solo el driver puede acceder a su valor\n",
    "\n",
    "-   Acumuladores usados en transformaciones de RDDs pueden ser incorrectos\n",
    "\n",
    "    -   Si el RDD se recalcula, el acumulador puede actualizarse\n",
    "\n",
    "    -   En acciones, este problema no ocurre\n",
    "\n",
    "-   Por defecto, los acumuladores son enteros o flotantes\n",
    "    - Es posible crear “acumuladores a medida” usando [`AccumulatorParam`](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html#pyspark.AccumulatorParam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from numpy.random import randint\n",
    "\n",
    "npares = sc.accumulator(0)\n",
    "\n",
    "def esPar(n):\n",
    "    global npares\n",
    "    if n%2 == 0:\n",
    "        npares += 1\n",
    "\n",
    "rdd = sc.parallelize(randint(100, size=10000))\n",
    "rdd.foreach(esPar)\n",
    "\n",
    "print(\"N pares: %d\" % npares.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables de broadcast\n",
    "\n",
    "-   Por defecto, todas las variables compartidas (no RDDs) son enviadas a todos los ejecutores\n",
    "\n",
    "    -   Se reenvían en cada operación en la que aparezcan\n",
    "\n",
    "-   Variables de broadcast: permiten enviar de forma eficiente variables de solo lectura a los workers\n",
    "\n",
    "    -   Se envían solo una vez\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "# dicc es una variable de broadcast\n",
    "dicc=sc.broadcast({\"a\":\"alpha\",\"b\":\"beta\",\"c\":\"gamma\"})\n",
    "\n",
    "rdd=sc.parallelize([(\"a\", 1),(\"b\", 3),(\"a\", -4),(\"c\", 0)])\n",
    "reduced_rdd = rdd.reduceByKey(add).map(lambda (x,y): (dicc.value[x],y))\n",
    "\n",
    "print(reduced_rdd.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabajando a nivel de partición\n",
    "\n",
    "Una operación map se hace para cada elemento de un RDD\n",
    "\n",
    "-   Puede implicar operaciones redundantes (p.e. abrir una conexión a una BD)\n",
    "\n",
    "-   Puede ser poco eficiente\n",
    "\n",
    "Se pueden hacer `map` y `foreach` una vez por partición:\n",
    "\n",
    "-   Métodos `mapPartitions()`, `mapPartitionsWithIndex()` y `foreachPartition()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "nums = sc.parallelize([1,2,3,4,5,6,7,8,9], 2)\n",
    "print(nums.glom().collect())\n",
    "\n",
    "def sumayCuenta(iterador):\n",
    "    sumaCuenta = [0,0]\n",
    "    for i in iterador:\n",
    "        sumaCuenta[0] += i\n",
    "        sumaCuenta[1] += 1\n",
    "    return sumaCuenta\n",
    "    \n",
    "print(nums.mapPartitions(sumayCuenta).glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "def sumayCuentaIndex(index, it):\n",
    "    return \"Particion \"+str(index), sumayCuenta(it)\n",
    "\n",
    "print(nums.mapPartitionsWithIndex(sumayCuentaIndex).glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "def f(iterator):\n",
    "    tempfich, _ = tempfile.mkstemp(dir=tempdir)\n",
    "    for x in iterator: \n",
    "        os.write(tempfich, str(x)+'\\t')\n",
    "    os.close(tempfich)\n",
    "        \n",
    "tempdir = \"/tmp/foreachPartition\"\n",
    "\n",
    "if not os.path.exists(tempdir):\n",
    "    os.mkdir(tempdir)\n",
    "    nums.foreachPartition(f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%sh\n",
    "TEMP=/tmp/foreachPartition\n",
    "echo \"Ficheros creados\"\n",
    "ls -l $TEMP\n",
    "echo\n",
    "echo \"Contenido de los ficheros\"\n",
    "for f in $TEMP/*;do cat $f; echo; echo \"====\"; done\n",
    "rm -rf $TEMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea\n",
    "\n",
    "Desarrollar un script PySpark, que, a partir de los ficheros secuencia en `apat63_99-seq` cree  un RDD clave valor, en el cual la clave es un país y el valor una lista de tuplas, en la que cada tupla esté formada por un año y el número de patentes de ese país en ese año. Además, debeis utilizar el contenido del fichero `country_codes.txt` (localizado en `../datos/country_codes.txt`) como una variable de broadcast y substituir el código del país por su nombre. Por último, el RDD creado debe estar ordenado por el nombre del país y, para cada país, la lista de valores ordenados por año.\n",
    "\n",
    "Recordad que cada registro de `apat63_99-seq` tiene un par clave valor `(país  patente,año)`, siendo tanto la clave como el valor de tipo *org.apache.hadoop.io.Text*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2.0.0",
   "language": "python",
   "name": "spark2"
  },
  "language_info": {
   "codemirror_mode": "text/python",
   "file_extension": ".py",
   "mimetype": "text/python",
   "name": "scala",
   "pygments_lexer": "python",
   "version": "3.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
